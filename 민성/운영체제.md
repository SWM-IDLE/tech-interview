### 프로세스와 스레드의 차이

- 프로세스
    - 컴퓨터에서 실행중인 프로그램
    - 각자 고유한 공간과 자원을 할당받는다.
    - 각각의 code, data, stack, heap 영역을 보유
    - 각각의 영역을 갖기에 동기화 작업이 필요하지 않음
    - 각각의 영역을 갖기에 컨텍스트 스위칭 비용이 크다.
- 쓰레드
    - 한 프로세스에서의 실행 단위
    - stack만 고유한 영역을 갖고 나머지 영역은 프로세스 자원을 공유
    - stack 이외의 영역을 공유하므로 동기화 작업이 필요
    - stack 이외의 영역을 공유하므로 컨텍스트 비용이 적다.

<hr/>

### 멀티프로세스 vs 멀티스레드

- 멀티프로세스
    - 하나의 프로그램을 여러 프로세스로 구성하여 프로세스가 병렬적으로 작업을 수행하는 것
    - 하나의 프로세스가 죽더라도 다른 프로세스에 영향을 끼치지 않는다.
    - 멀티 쓰레드보다 더 많은 메모리 공간을 차지하기 때문에 컨텍스트 스위칭이 느리다.
- 멀티쓰레드
    - 하나의 프로세스를 여러 개의 스레드로 구성하여 자원을 공유하면서 작업을 수행
    - 멀티 프로세스보다 적은 메모리 공간을 차지하여 컨텍스트 스위칭이 빠르다.
    - 자원을 공유하고 있기 때문에 하나의 스레드에 오류가 생겨 종료되면 모든 스레드가 종료될 수 있다.
    - 동기화 문제를 가지고 있다.
- 멀티프로세스 대신 멀티 프로세스를 사용하는 이유?
    - 프로그램을 여러 개 키는 것보다 하나의 프로그램 안에서 여러 작업을 해결하는 것이 더욱 효율적
    - 컨텍스트 스위칭 시 stack 영역만 초기화하면 되기 때문에 빠르다.
    - 프로세스를 생성하여 자원을 할당하는 콜이 줄어들어 자원을 효율적으로 관리
- 쓰레드마다 스택을 독립적으로 할당하는 이유
    - 스택 메모리 공간이 독립적 → 독립적 함수 호출이 가능하다 → 독립적인 실행 흐름

<hr/>

### 동기와 비동기의 차이

- Blocking
    - 자신의 작업을 진행하다가 다른 주체의 작업이 시작되면 다른 작업이 끝날 때까지 기다렸다가 자신의 작업을 시작
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 다 읽을 때까지 직원에게 기다려라 ~
        - 상사가 다 읽고 나서 돌아가라 ~
        - 직원은 다시 자기의 일을 할 수 있다.
- Non-Blocking
    - 다른 주체의 작업에 관련없이 자신의 작업을 하는 것
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 받고 읽어볼테니 돌아가세요 ~
        - 직원은 다시 자기 일을 할 수 있다.

**💡 다른 주체가 작업을 할 때 자신의 제어권이 있는지 없는지로 볼 수 있다.**

- Synchronous (동기)
    - 작업을 동시에 수행하거나, 동시에 끝나거나, 끝나는 동시에 시작함을 의미
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 직원이 기다리거나 다른일을 해도 신경쓰지 않는다.
        - 상사가 전달받은 서류를 다 읽고 나서 결과를 돌려주면 사원은 바로 처리한다.
- Asynchronous (비동기)
    - 시작, 종료가 일치하지 않으며, 끝나는 동시에 시작하지 않음을 의미
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 직원이 기다리거나 다른일을 해도 신경쓰지 않는다. 또한 일을 바로 처리하지 않아도 됨
        - 상사가 전달하면, 사원은 나중에 내용을 확인하고 처리하면 됨.


**💡 결과를 돌려주었을 때 순서와 결과에 관심이 있는지 아닌지로 판단할 수 있다.**

<img width="505" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/6a8ba6c6-62df-429f-a871-a5aa9dc6280e">

- Blocking/Sync
    - 다른 작업이 시작되는 동안 동작하지 X → Blocking
    - 결과 반환하면 해당 업무를 바로 처리  → Sync
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 다 읽을 때까지 직원에게 기다려라 ~ (Blocking)
        - 다 읽고 사원에게 전달하면 사원은 해당 업무를 바로 처리 (Sync)
    - ex) 자바에서 입력요청, 입력하면 입력값을 처리
- Non-Blocking/Sync
    - 다른 작업 있어도 자신의 제어권을 가지고 일을 한다 → Non-Blocking
    - 중간중간 결과를 물어본다, 결과 반환하면 해당 업무를 바로 처리→ Sync
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 받고 읽어볼테니 사원에게 다른 업무를 보라고 한다 ~ (Non-Blocking)
        - 업무를 보는 중간 중간 끝났냐고 물어봄, 결과가 나오면, 결과를 가지고 바로 처리 (Sync)
    - Blocking/Sync와 큰 차이가 없다.
        - ex) 게임에서 맵을 넘어갈때의 예시 ⇒ 해당 데이터를 가져올때까지 로드율을 사용자에게 보여줘야함.
- Blocking/Async
    - 다른 작업이 시작되는 동안 동작하지 X → Blocking
    - 결과가 나와도 바로 처리하지 않아도 됨 → Async
    
    ⇒ 굳이 비동기인데, Blocking을 사용할까?
    
    - Non-Blocking/Async로 하려다가 개발자의 실수, 기타 이유로 동작할 수 있다.
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 다 읽을 때까지 직원에게 기다려라 ~ (Blocking)
        - 결과에 관심은 없지만 기다리고 있음 (Async)
        - 일이 끝나면 상사는 메일로 결과를 넘겨주고, 돌아가라 함
        - 사원 돌아가서 일을 하다가 시간이 되면 상사에게 전달 받은 일을 처리함 (Async)
- Non-Blocking/Async
    - 다른 작업 있어도 자신의 제어권을 가지고 일을 한다 → Non-Blocking
        - 양쪽에서 서로 각자 작업을 처리
    - 다른 작업의 결과가 나와도 바로 처리하지 않아도 됨 → Async
    - ex)
        - 직원이 서류를 상사에게 전달
        - 상사는 해당 서류를 받고 읽어볼테니 돌아가세요, 직원은 다시 자기 일을 할 수 있다. (Non-Blocking)
        - 메일에 결과를 담아서 전달, 사원은 나중에 메일을 확인하고 처리 (Async)
    - ex) 자바스크립트에서 API 요청을 하고 다른 작업을 하다가 콜백을 통해서 추가적인 작업을 처리할 때

<hr/>

### 스케줄러의 종류

- 장기 스케줄러 (어떤 프로세스를 ready queue에 보낼지)
    - 디스크와 메모리 사이의 스케줄링을 관리
    - 디스크에 비해 메모리 크기는 한정되어 있음으로 수행해야할 일에서 선택적으로 **프로세스에 메모리를 할당하여 Ready Queue로 보내는** 역할
- 단기 스케줄러 (어떤 프로세스를 실행시킬지)
    - 메모리와 CPU 사이의 스케줄링 관리
    - Ready Queue에서 어떤 프로세스를 Running 할지 결정
- 중기 스케줄러 (메모리 공간이 부족한 경우 어떤 프로세스를 swap out할지)
    - 너무 많은 프로세스가 메모리에 올라와있을 경우 이를 다시 디스크로 보낸다.
    - CPU를 사용하려는 프로세스들이 사이에서 중재하여 일시 보류, 재활성화를 수행한다.

<hr/>

### CPU 스케줄러

- 비선점 방식
    - FCFS
        - 먼저 CPU를 요청하는 프로세스를 먼저 처리하는 방식
        - 최악의 경우 오래 걸리는 문제가 가장 먼저 들어옴
    - SJF
        - CPU 점유 시간이 가장 짧은 프로세스에 CPU를 먼저할당하는 방식
            - CPU 점유 시간 = CPU Burst Time : CPU에 직접 돌리기 전까지 알 방법이 없다. ⇒ 실제값이 아닌 예측값
            - https://charles098.tistory.com/82
        - 기아 문제 발생
            - Aging 기법 : 특정 프로세스의 우선순위가 낮아 무한정 기다리게 되는 경우, 일정시간이 지나면 우선순위를 한 단계씩 높여 가까운 시간 안에 자원을 할당받도록 하는 기법
- 선점 방식
    - SRT(F)
        - CPU 점유 시간이 가장 짧은 프로세스에 CPU를 먼저 할당하는 방식
    - Priority scheduling
        - 우선 순위가 높은 프로세스에 CPU를 우선 할당하는 방식의 스케줄링
        - 우선 순위는 시간 제한, 메모리 요구량, 프로세스의 중요성, 자원사용 비용 등에 따라 달라진다.
    - RR
        - 시간 할당량을 매 프로세스에 주고 할당된 시간안에 완료하지 못한 프로세스는 Ready Queue의 맨 뒤에 배치하는 방식

<hr/>

### 프로세스 동기화

- Race Condition
    - 여러 프로세스들이 동시에 데이터에 접근하는 상황에서, 어떤 순서로 데이터에 접근하느냐에 따라 결과 값이 달라질 수 있는 상황
    - Race Condition을 막고 일관성을 유지하기 위해서 프로세스의 실행 순서를 정해주는 메커니즘인 동기화가 필요
    - 발생할 수 있는 경우
        - 커널 모드로 수행 중 인터럽트 발생
        - 프로세스가 시스템 콜을 호출해서 커널 모드로 수행 중인데 Context Switch가 발생
        - 멀티 프로세서에서 공유 메모리 내의 커널 데이터에 접근
- Critical Section (임계 구역)
    - 코드 상에서 Race Condition이 발생할 수 있는 특정 부분, 즉 공유 데이터를 접근하는 코드 부분
    - 조건
        - Mutual Exclusion (상호 배제)
        - Progress (진행)
        - Bounded Waiting (한정 대기)
- 해결책
    - Mutex Locks
        - lock이 하나만 존재할 수 있는 locking 메커니즘
        - 이미 하나의 프로세스가 Critical Section에서 작업 중이면 다른 프로세스들은 Critical Section에 들어갈 수 없다.
        - Critical Section에 프로세스가 존재할 때, 다른 프로세스들은 계속 진입 시도를 하기 때문에 CPU를 낭비 → Busy Waiting
        - Spin Lock : lock이 반환될 때까지 계속 확인하면서 프로세스가 기다리는 것
    - Semaphores
        - Busy Waiting이 필요 없는 동기화 도구, 여러 프로세스나 스레드가 Critical Section에 진입할 수 있는 Signaling 메커니즘
        - 카운터를 이용하여 동시에 자원에 접근할 수 있는 프로세스를 제한
        - 종류
            - Counting Semaphore : 정수 값의 범위가 0 이상으로 제한이 없다.
            - Binary Semaphore : 정수 값이 오직 0 또는 1 이다. (Metex lock과 동일)
        - Block & Wakeup 방식 사용 : Critical Section으로의 진입에 실패한 프로세스는 기다리게 하지 않고 Block 시킨 뒤 Critical Section에 자리가 나면 다시 깨워줌으로써 Busy Waiting에서의 CPU 낭비를 해결
            
            ⇒ Critical Section이 매우 짧은 경우 Block & Wakeup의 오버헤드가 더 커질 수 있다.
            
    - Monitor
        - 이전 세마포어의 문제점은 코딩하기 힘들고 실수하기 쉬우며, 정확성을 입증하기 어렵다.
        - 동시 수행 중인 프로세스 사이에서 추상 데이터의 안전한 공유를 보장하기 위한 High-level 동기화 구조. 공유 데이터를 접근하기 위해서는 모니터의 내부 Procedure를 통해서만 접근할 수 있다.
        <img width="507" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/37bbaf1e-3663-4e46-ba29-6e9b076afe2b">

    - 데드락
        - 일련의 프로세스들이 서로가 가진 자원을 기다리며 block 되어 더이상 진행이 될 수 없는 상태
        - 조건
            - 상호배제
                - 매 순간 하나의 프로세스만이 자원을 사용할 수 있다.
                - 하나의 자원은 하나의 프로세스만 점유
            - 비선점
                - 프로세스는 OS에 의해 강제로 자원을 빼앗기지 않는다.
            - 점유대기
                - 자원을 가진 프로세스가 다른 자원을 기다릴 때, 보유하고 있는 자원을 놓지 않고 계속 가지고 있다.
            - 순환대기
                - 자원을 기다리는 프로세스 간에 사이클이 형성되어야 한다.
        - Deadlock Prevention (예방)
            - 미리 예방하는 방식으로 자원을 할당할 때 Deadlock의 4가지 필요조건 중 어느 하나가 만족되지 않도록 하는 방식
            - Mutual Exclusion
                - Critical Section Problem을 해결하기 위해서는 이 조건은 반드시 필요하다. → 상호배제는 항상 만족시켜야 한다.
            - Hold and Wait (점유 대기 만족 X)
                - 프로세스가 자원을 요청할 때 다른 어떤 자원도 가지고 있지 않도록 해야 한다.
            - No preemption
                - 프로세스가 어떤 자원을 기다려야 하는 경우 보유하고 있던 자원이 선점된다.
            - Circular wait
                - 자원의 타입에 따라 프로세스마다 할당 순서를 정하여 정해진 순서대로만 자원을 할당한다.
            
            ⇒ Deadlock을 미리 방지하는 방식은 효율성과 처리량을 감소시키고, Starvation이 발생할 수 있다.
            
        - Deadlock Avoidance (회피)
            - Deadlock이 발생할 가능성이 있는 경우엔 아예 자원을 할당하지 않는 방식
            - 프로세스들이 필요로 하는 각 자원별 최대 사용량을 미리 선언하도록 하는 방식
            - Avoidance 알고리즘이 있는데, 각 자원 타입마다 하나의 인스턴스가 존재하는 경우 자원 할당 그래프 알고리즘을 사용하고, 여러 인스턴스가 존재하는 경우 Banker's 알고리즘을 사용한다.
                - Resource Allocation Graph Algorithm
                - Banker’s Algorithm
        - Deadlock Detection and Recovery (탐지와 회복)
            - Detection
                - 그래프를 통해서 사이클이 생겼는지 확인하거나, 총 요청 자원의 수가 남은 자원의 수보다 적은 프로셋가 존재하지 않는다는 것을 이용하여 확인
            - Recovery
                - 프로세스를 종료시키거나 자원을 선점하는 방법
                - 종료
                    - Deadlock에 빠진 모든 프로세스를 종료
                    - Deadlock이 해결될 때까지 한 번에 한 프로세스씩 종료
                - 선점
                    - 어떤 프로세스를 종료시킬지 결졍하고, Deadlock이 발생하기 전 상태로 돌아가 프로세스를 재시작
                    - 프로세스가 계속해서 victim으로 선정되는 경우 기아 현상이 발생할 수 있어 Rollback 된 횟수를 저장함으로써 해결 가능
        - Deadlock Ingnore
            - Deadlock이 일어나지 않는다고 생각하고 아무런 조치도 취하지 않는 방식
            - Deadlock이 매우 드물게 발생하기 때문에, Deadlock에 대한 조치 자체가 더 큰 오버헤드일 수 있기 때문
            - Deadlock 발생시, 시스템이 비정상적으로 작동하는 것을 사람이 느낀 후 직접 프로세스를 죽이는 등으로 대처한다.
            - 범용 운영체제가 채택하는 방식이다.

<hr/>

### 메모리 관리 전략

- 메모리 관리 배경
    - **Address Binding**
        - 프로세스 주소는 논리적 주소와 물리적 주소로 나뉜다.
            - 논리적 주소 = 가상 주소
                - CPU가 생성하는 주소
                - 프로세스마다 독립적으로 가지는 주소 공간이기 때문에, 프로세스의 내부에서 사용하고 각 프로세스마다 0 부터 시작
            - 물리적 주소
                - 프로세스가 실행되기 위해 실제로 메모리에 올라가는 위치
        - Compile Time, Load Time 주소 할당은 실제로 잘 쓰이지 않는다.
        - Execution Time (Run Time)
            - 프로세스가 수행이 시작된 이후에 프로세스가 실행될 때 메모리 주소를 바꾸는 방법이다.
            - Runtime 때 물리적 주소가 결정되며 실행 도중 주소가 바뀔 수 있다.
            - MMU (Memory Management Unit) 라는 하드웨어 장치를 사용하여 논리적 주소를 물리적 주소로 바꿔준다.
    - **Swapping**
        - 메모리는 크기가 크지 않기 때문에 프로세스를 임시로 디스크에 보냈다가 다시 메모리에 로드해야 하는 상황이 생긴다.
        - 디스크로 내보는 것을 swap out, 메모리로 들여보내는 것을 swap in
        - 중기 스케줄러에 의해 swap out 시킬 프로세스를 선정
    - **Contiguous Allocation**
        - 메모리는 일반적으로 커널 영역과 사용자 프로세스 영역으로 나뉘어 사용한다.
        - 사용자 프로세스 영역의 할당 방법을 연속적 할당, 비연속적 할당으로 나뉜다.
        - Contiguous Allocation
            - 각 프로세스들이 연속적인 메모리 공간을 차지하게 되는 것
            - 각 프로세스를 메모리에 담기 위해 메모리는 미리 공간을 분할해두는데, 고정된 크기로 나누는 고정 분할 방식과 프로세스의 크기를 고려해서 나누는 가변 분할 방식이 있다.
            <img width="504" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/d861f92e-08d4-4fe1-b8a5-b9fcd4317222">

            - 각 프로세스를 메모리에 담기 위해 메모리는 미리 공간을 분할해두는데, 고정된 크기로 나누는 고정 분할 방식과 프로세스의 크기를 고려해서 나누는 가변 분할 방식이 있다.
            - Contiguous Allocation에서 메모리를 분할하는 각 단위는 **Block**이고, 프로세스가 사용할 수 있는 Block을 **Hole**이라고 한다.
                - 가변 분할 방식에서 크기가 n인 프로세스가 들어갈 가장 적절한 Hole을 찾는 문제를 Dynamic Storage-Allocation Problem이라고 한다.
                    - First-fit , Best-fit, Worst-fit
    - **Fragmentation(단편화)**
        - 프로세스들이 메모리에 적재되고 제거되는 일이 반복되면 프로세스들이 차지하는 메모리 틈 사이에 사용하지 못할 만큼의 작은 공간들이 늘어나는 현상
        - 외부 단편화
            - 총 공간을 계산했을 때 프로세스가 들어갈 수 있는 메모리가 있음에도 불구하고 공간들이 연속하지 않아 사용할 수 없는 경우
        - 내부 단편화
            - 프로세스가 사용하는 메모리 공간보다 분할된 공간이 더 커서 메모리가 남은 경우
            - 1000 크기의 분할이 있고 990 크기의 프로세스가 들어갈 때, 10만큼의 공간이 남게 되는 현상
            <img width="535" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/d74edbd5-3ce7-4862-a632-52892537768b">

        - 고정 분할은 외부 단편화와 내부 단편화가 모두 발생할 수 있지만, 가변 분할은 외부 단편화가 발생할 수 있다. ????
        - 외부 단편화를 해결할 수 있는 방법으로는 Compaction(압축)이 있다. 프로세스가 사용하는 공간들을 한쪽으로 몰아서 공간을 확보하는 방법이지만, 비용이 매우 많이 드는 작업이라 효율이 좋지 않다.
- **Paging**
    - Noncontiguous Allocation 방식
    - 외부단편화의 압축 작업의 비효율성을 해결하기 위한 방법, 메모리는 프레임, 프로세스는 페이지라 불리는 고정 크기의 블록으로 분리된다. 물리적 메모리에 불연속적을 저장하는 방식
    - 한 프로세스가 사용하는 공간은 여러 page로 나뉘어 관리되고, 각각의 page는 순서와 관계없이 메모리의 frame에 mapping 되어 저장된다.
    - 프로세스가 순서대로 메모리에 저장되지 않기 때문에 프로세스를 실행하기 위해서는 page가 어느 frame에 들어있는지 알아야 한다.
        - 이에 대한 정보는 page table이라는 테이블에 저장되어 있고, 이를 사용하여 논리적 주소를 물리적 주소로 변환한다.
    - 장점
        - page들이 연속할 필요가 없어 외부 단편화를 해결할 수 있다.
        - 할당과 해제가 빠르다, swap out이 간단하다.
    - 단점
        - 내부 단편화는 해결하지 못한다.
        - page table을 저장하기 위한 메모리가 추가로 소모된다.
        - page table이 메모리에 상주하기 때문에 메모리에 접근하는 연산은 2번의 메모리 접근이 필요하게 되어 속도가 느리다.
        
        → 속도향상을 위해 TLB(Translation Look-aside Buffer)라 불리는 고속의 하드웨어 캐시를 사용한다.
        
        - TLB : 메모리 주소 변환을 위한 별도의 캐시 메모리로, page table에서 빈번히 참조되는 일부 엔트리를 caching 하고 있다.     
        <img width="485" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/13226fd6-c79a-4a4b-aed1-17c738bf550c">

    - page의 크기가 작아질수록 내부 단편화가 감소하고 필요한 정보만 메모리에 있어서 메모리 이용에서 효율적이지만, page table의 크기가 증가하고 디스크 이동의 효율성이 감소한다.
- **Segmentation**
    - 의미 있는 단위로 하나의 프로세스를 나누는 것
    - 프로세스의 주소공간을 서로 크기가 다른 논리적인 블록단위인 세그먼트로 분할하고 메모리에 배치하는 방식
    - 작게는 프로그램을 구성하는 함수 하나하나를, 크게는 프로그램 전체를 하나의 Segment로 정의, 일반적으로 code, data, stack 부분이 하나의 세그먼트로 정의
    - 페이징과 비슷하게 Segment Table을 가지고 있지만, 세그먼트의 크기가 일정하지 않기 때문에, limit 정보를 추가로 들어있다.
    - 외부 단편화가 발생한다.

<hr/>

### 가상 메모리

- 배경
    - 기존에는 프로세스가 실행되는 코드의 전체를 메모리에 로드해야 했고, 메모리 용량보다 더 큰 프로그램은 실행시킬 수 없었다. 하지만 실제로는 코드의 일부에서만 대부분의 시간을 사용하고, 프로세스는 특정 순간에는 항상 작은 양의 주소 공간을 사용했기 때문에 이러한 방식은 매우 비효율적이었다.
- 가상 메모리가 하는 일
    - 가상 메모리란?
        - 물리적 메모리 크기의 한계를 극복하기 위해 나온 기술
        - 프로세스를 실행할 때 실행에 필요한 일부만 메모리에 로드하고 나머지는 디스크에 두는 것
        - 현재 필요한 page만 메모리에 올리는 것을 Demand Paging이라 함.
- Demand Paging (요구 페이징)
    - 실제 필요한 page를 메모리에 올리는 것
    - CPU 이용률과 처리율이 높아지고, 더 많은 사용자를 수용할 수 있다.
- 페이지 교체 알고리즘
    - OPT (Optimal Algorithm)
        - 가장 먼 미래에 참조되는 page를 대체하는 방법
        - 항상 최적의 결과를 갖는다.
        - 미래의 참조를 모두 알고 있어야 하므로 실제로 사용하기는 아렵고, 다른 알고리즘의 성능에 대한 upper bound를 제공한다.
    - FIFO (First In First Out)
        - 제일 먼저 들어온 것을 먼저 내쫓는 방법
        - 어떤 page는 항상 필요할 수 있는데, 그런 경우에도 replace가 되는 단점이 있다.
        - frame이 늘어나도 page fault가 감소하지 않고 오히려 늘어나는 경우가 존재 = Belady’s anomaly
        <img width="509" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/912c453e-a361-4bb6-8a7f-54789e89e529">
    - LRU (Least Recently Used)
        - 가장 오래전에 참조된 것을 지우는 방법 (Optimal에 근접)
        - 구현하기 어렵고 접근되는 빈도를 고려하지 않는 단점이 있다.
        <img width="510" alt="image" src="https://github.com/SWM-IDLE/tech-interview/assets/71378475/f26e8bfa-30fd-4767-b58d-4dace643c457">

    - LFU (Least Frequently Used)
        - 참조 횟수가 가장 적은 page를 지우는 방법
        - LRU에 비해 장기적인 시간 규모를 보기 때문에 page의 인기도를 조금 더 정확히 반영할 수 있지만, 최근성은 반영하지 못한다.
    - Second Chance Algorithm (Clock Algorithm)
        - LRU와 LFU는 실제로 paging system에서 사용할 수 있을까 ? X
            - LRU와 LFU의 알고리즘 구현에서 운영체제가 자료구조를 변경하고 유지하는 작업을 수행해야 하는데, 이미 메모리에 page가 올라가 있는 경우에는 CPU가 운영체제에 넘어가지 않는다. page fault가 발생해야 CPU가 운영체제에 넘어가고, 디스크에서 메모리로 page를 로드할 때 해당 page에 대한 정보를 얻고 갱신할 수 있다.
            - 따라서 운영체제가 참조한 지 오래되거나 참조 횟수가 적은 페이지를 정확하게 알 수 없다.
        - Second Chance Algorithm은 LRU의 근사 알고리즘으로, 최근에 참조되었는지 여부를 나태내는 Reference bit이라는 정보를 사용

<hr/>

### 캐시의 지역성
- **캐시 메모리**
    - 속도가 빠른 장치와 느린 장치 간의 속도 차에 따른 병목 현상을 줄이기 위한 범용 메모리.
    - 메인 메모리와 CPU(Register) 사이에 위치
    - 메인 메모리에서 자주 사용하는 프로그램과 데이터를 저장해두어 속도를 빠르게 한다.
        - 이를 위해 CPU가 어떤 데이터를 원하는지 어느 정도 예측할 수 있어야 한다.
        - 작은 크기의 캐시 메모리에 CPU가 이후에 참조할 정보가 어느 정도 들어있는지에 따라 캐시의 성능이 결정되기 때믄
        
        ⇒ 캐시의 지역성 이용
        
- **Locality**
    - 데이터에 대한 접근이 시간적 혹은 공간적으로 가깝게 발생하는 것.
    - 캐시의 Hit rate을 극대화하여 캐시가 효율적으로 동작하기 위해 사용되는 성질
    - 공간 지역성
        - 최근에 사용했던 데이터와 인접한 데이터가 참조될 가능성이 높다.
        - ex) 배열의 예, A[0], A[1]과 같은 연속 접근의 경우 그다음 원소들에 접근할 가능성이 높다
    - 시간 지역성
        - 최근에 사용했던 데이터가 재참조될 가능성이 높다.
        - ex) for, while 같은 반복문을 예로 들어, 특정 부분을 반복해서 접근하기 때문에 다시 참조할 확률이 높다.
- **Caching line**
    - 배경
        - 캐시메모리는 메인메모리에 비해 크기가 매우 작기 때문에 메인메모리와 1대1 매칭이 불가능하다.
        - 캐시가 아무리 CPU에 가깝게 위치하더라도, 데이터가 캐시 내의 어느 곳에 저장되어 있는지 찾기가 어려워 모든 데이터를 순회해야 한다면 캐시의 장점을 잃게 때문에 쉽게 찾을 수 있는 구조가 필요
    - 캐시 데이터를 저장할 때 특정 자료구조를 사용하여 묶음으로 저장 = 캐싱 라인
        - Direct Mapping
            - 직접 매핑으로, 메인 메모리를 일정한 크기의 블록으로 나누어 각각의 블록을 캐시의 정해진 위치에 매핑하는 방식
            - Hit rate가 낮을 수 있다. 또 동일한 캐시 메모리에 할당된 여러 데이터를 사용할 때 충돌이 발생
        - Full Associative Mapping
            - 캐시 메모리의 빈 공간에 마음대로 주소를 저장하는 방식
            - 저장 방식은 간단하지만, 원하는 데이터가 있는지 찾기 위해서는 모든 태그를 병렬적으로 검사해야 하기 때문에 복잡하고 비용이 높다.
        - Set Associative Mapping
            - Direct Mapping과 Full Associative Mapping의 장점 결합
            - 빈 공간에 마음대로 주소를 저장하되, 미리 정해둔 특정 행에만 저장하는 방식
            - Direct에 비해 검색은 느리지만 저장은 빠르고, Full 보다 저장은 느리지만 검색은 빠르다.
            - 주로 사용
    - **Cache Miss**
        - CPU가 참조하려는 데이터가 캐시 메모리에 없을 때 발생
        - Compulsory Miss
            - 특정 데이터에 처음 접근할 때 발생하는 cache miss
        - Capacity Miss
            - 캐시 메모리의 공간이 부족해서 발생하는 cache miss
        - Confilct Miss
            - 캐시 메모리에 A와 B 데이터를 저장해야 하는데, A와 B가 같은 캐시 메모리 주소에 할당되어 있어서 발생하는 cache miss이다. direct mapped cache에서 많이 발생한다.
